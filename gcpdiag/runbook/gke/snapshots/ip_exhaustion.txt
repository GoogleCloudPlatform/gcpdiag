end_time=2024-06-30T23:00:00Z,location=us-central1-c,name=cluster-1,project_id=gcpdiag-
gke3-gggg,start_time=2024-06-30T01:00:00Z

gke/ip-exhaustion: Troubleshooting ip exhaustion issues on GKE clusters.

  This runbook investigates the gke cluster for ip exhaustion issues and recommends remediation
  steps.

  Areas Examined:
  - GKE cluster type.
  - GKE cluster and nodepool configuration
  - Stackdriver logs
    
[START]: Starting the IP Exhaustion diagnostics

   - gcpdiag-gke3-gggg                                                    [OK]
     [REASON]
     Cluster cluster-1 found in us-central1-c for project gcpdiag-gke3-gggg
[AUTOMATED STEP]: Checking node IP Exhaustion and offering remediation steps
[INFO]: Searching cloud logging for the string IP_SPACE_EXHAUSTED which indicates IP Exhaustion issue
[INFO]: IP_SPACE_EXHAUSTED error found for cluster cluster-1 and subnet public-subnet

   - gcpdiag-gke3-gggg/us-central1-c/cluster-1                            [FAIL]
     [REASON]
     Node IP exhaustion is detected in the cluster cluster-1 for the subnet public-subnet


     [REMEDIATION]
     Please follow the below documentation [1] to expand the ip range of the node subnet.
     The subnet that has exhausted its IP space is public-subnet.

     [1] https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet

[AUTOMATED STEP]: Checking Pod IP Exhaustion and offering remediation steps
[INFO]: Verifying if the cluster is an Autopilot cluster or a Standard cluster.
[INFO]: Cluster is a Standard cluster

   - gcpdiag-gke3-gggg/us-central1-c/cluster-1                            [FAIL]
     [REASON]
     Pod IP exhaustion is detected in the cluster cluster-1

     [REMEDIATION]
     Here are some commands you can run to add an additional IPv4 pod range to address this issue, and you can find more detailed instructions at the documentation [1].

     **Allocate IP addresses with designed length and create a pod range**

     ```
     export POD_IP_RANGE_PREFIX_LENGTH=14
     export POD_RANGE_NAME="range-`tr -dc a-z0-9 </dev/urandom | head -c 13`"
     gcloud network-connectivity internal-ranges create $POD_RANGE_NAME \
         --network=projects/gcpdiag-gke3-gggg/global/networks/default \
         --prefix-length=$POD_IP_RANGE_PREFIX_LENGTH
     ```

     **Assign the pod range to subnet**

     ```
     gcloud compute networks subnets update public-subnet \
         --region=us-central1-c \
         --add-secondary-ranges-with-reserved-internal-range="$POD_RANGE_NAME=//networkconnectivity.googleapis.com/projects/gcpdiag-gke3-gggg/locations/global/internalRanges/$POD_RANGE_NAME"
     ```

     **Add additional pod range to your cluster**

     ```
     gcloud container clusters update cluster-1 \
         --additional-pod-ipv4-ranges=$POD_RANGE_NAME \
         --location=us-central1-c
     ```

     Following execution of these commands, you should be able to create a new node pool that utilizes the newly configured pod IP range without encountering pod IP exhaustion issues.

     Additionally, consider leveraging the Class E IPv4 address space (240.0.0.0/4) to support your growth.
     While these addresses are reserved for future use (see Google VPC network valid IPv4 ranges [2]), they can be used in certain situations.

     The linked documentation [3] addresses common misconceptions about Class E, discusses its benefits and considerations, and provides guidance on planning and using GKE clusters with this address space.
     It also includes a real-world example of how Class E successfully solved IP exhaustion challenges.

     [1] https://cloud.google.com/kubernetes-engine/docs/how-to/multi-pod-cidr#cluster-add-pod-ipv4-range

     [2] https://cloud.google.com/vpc/docs/subnets#valid-ranges

     [3] https://cloud.google.com/blog/products/containers-kubernetes/how-class-e-addresses-solve-for-ip-address-exhaustion-in-gke


[END]: Finalize VM external connectivity diagnostics.


