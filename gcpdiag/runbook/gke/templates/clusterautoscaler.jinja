{% block out_of_resources_failure_reason %}
The scaleUp event failed because some of the MIGs could not be increased due to lack of resources.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock out_of_resources_failure_reason %}

{% block out_of_resources_failure_remediation %}
Follow the documentation:
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-vm-creation#resource_availability
{% endblock out_of_resources_failure_remediation %}

{% block out_of_resources_success_reason %}
No "scale.up.error.out.of.resources" errors found between {start_time} and {end_time}
{% endblock out_of_resources_success_reason %}

{% block quota_exceeded_failure_reason %}
The scaleUp event failed because some of the MIGs could not be increased, due to exceeded Compute Engine quota.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock quota_exceeded_failure_reason %}

{% block quota_exceeded_failure_remediation %}
Check the Errors tab of the MIG in Google Cloud console to see what quota is being exceeded. Follow the instructions to
request a quota increase:
https://cloud.google.com/compute/quotas#requesting_additional_quota
{% endblock quota_exceeded_failure_remediation %}

{% block quota_exceeded_success_reason %}
No "scale.up.error.quota.exceeded errors" found between {start_time} and {end_time}
{% endblock quota_exceeded_success_reason %}

{% block instance_timeout_failure_reason %}
The scaleUp event failed because instances in some of the MIGs failed to appear in time.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock instance_timeout_failure_reason %}

{% block instance_timeout_failure_remediation %}
This message is transient. If it persists, engage Google Cloud Support for further investigation.
{% endblock instance_timeout_failure_remediation %}

{% block instance_timeout_success_reason %}
No "scale.up.error.waiting.for.instances.timeout" errors found between {start_time} and {end_time}
{% endblock instance_timeout_success_reason %}

{% block ip_space_exhausted_failure_reason %}
The scaleUp event failed because the cluster doesn't have enough unallocated IP address space to use to add new nodes or
Pods.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock ip_space_exhausted_failure_reason %}

{% block ip_space_exhausted_failure_remediation %}
Refer to the troubleshooting steps to address the lack of IP address space for the nodes or pods.
https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips#not_enough_space
{% endblock ip_space_exhausted_failure_remediation %}

{% block ip_space_exhausted_success_reason %}
No "scale.up.error.ip.space.exhausted" errors found between {start_time} and {end_time}
{% endblock ip_space_exhausted_success_reason %}

{% block service_account_deleted_failure_reason %}
The scaleUp event failed because a service account used by Cluster Autoscaler has been deleted.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock service_account_deleted_failure_reason %}

{% block service_account_deleted_failure_remediation %}
Engage Google Cloud Support for further investigation.
{% endblock service_account_deleted_failure_remediation %}

{% block service_account_deleted_success_reason %}
No "scale.up.error.service.account.deleted" errors found between {start_time} and {end_time}
{% endblock service_account_deleted_success_reason %}

{% block min_size_reached_failure_reason %}
Node cannot be removed because its node group is already at its minimum size.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock min_size_reached_failure_reason %}

{% block min_size_reached_failure_remediation %}
Review and adjust the minimum value set for node pool autoscaling.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler#resizing_a_node_pool
{% endblock min_size_reached_failure_remediation %}

{% block min_size_reached_success_reason %}
No "no.scale.down.node.node.group.min.size.reached" errors found between {start_time} and {end_time}
{% endblock min_size_reached_success_reason %}

{% block failed_evict_pods_failure_reason %}
The scaleDown event failed because some of the Pods could not be evicted from a node.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock failed_evict_pods_failure_reason %}

{% block failed_evict_pods_failure_remediation %}
Review best practices for Pod Disruption Budgets to ensure that the rules allow for eviction of application replicas
when acceptable.
https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#add-pod_disruption_budget-to-your-application
{% endblock failed_evict_pods_failure_remediation %}

{% block failed_evict_pods_success_reason %}
No "scale.down.error.failed.to.evict.pods" errors found between {start_time} and {end_time}
{% endblock failed_evict_pods_success_reason %}


{% block disabled_annotation_failure_reason %}
The scaleDown event failed because the node is annotated with cluster-autoscaler.kubernetes.io/scale-down-disabled:
true.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock disabled_annotation_failure_reason %}

{% block disabled_annotation_failure_remediation %}
Cluster autoscaler skips nodes with this annotation without considering their utilization and this message is logged
regardless of the node's utilization factor.
If you want cluster autoscaler to scale down these nodes, remove the annotation.
{% endblock disabled_annotation_failure_remediation %}

{% block disabled_annotation_success_reason %}
No "no.scale.down.node.scale.down.disabled.annotation" errors found between {start_time} and {end_time}
{% endblock disabled_annotation_success_reason %}


{% block min_resource_limit_exceeded_failure_reason %}
The scaleDown event failed because it would violate cluster-wide minimal resource limits.
These are the resource limits set for node auto-provisioning.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock min_resource_limit_exceeded_failure_reason %}

{% block min_resource_limit_exceeded_failure_remediation %}
Review your limits for memory and vCPU and, if you want cluster autoscaler to scale down this node, decrease the limits
by following the documentation
https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning#enable
{% endblock min_resource_limit_exceeded_failure_remediation %}

{% block min_resource_limit_exceeded_success_reason %}
No "no.scale.down.node.minimal.resource.limits.exceeded" errors found between {start_time} and {end_time}
{% endblock min_resource_limit_exceeded_success_reason %}


{% block no_place_to_move_pods_failure_reason %}
The scaleDown event failed because there's no place to move Pods.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock no_place_to_move_pods_failure_reason %}

{% block no_place_to_move_pods_failure_remediation %}
If you expect that the Pod should be rescheduled, review the scheduling requirements of the Pods on the underutilized
node to determine if they can be moved to another node in the cluster.
To learn more, see the link
https://cloud.google.com/kubernetes-engine/docs/troubleshooting/cluster-autoscaler-scale-down#no-place-to-move-pods
{% endblock no_place_to_move_pods_failure_remediation %}

{% block no_place_to_move_pods_success_reason %}
No "no.scale.down.node.no.place.to.move.pods" errors found between {start_time} and {end_time}
{% endblock no_place_to_move_pods_success_reason %}


{% block pod_not_backed_by_controller_failure_reason %}
The scaleDown event failed because a Pod is not backed by a controller such as ReplicationController, DaemonSet, Job,
StatefulSet, or ReplicaSet.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock pod_not_backed_by_controller_failure_reason %}

{% block pod_not_backed_by_controller_failure_remediation %}
Set the annotation "cluster-autoscaler.kubernetes.io/safe-to-evict": "true" for the Pod or define an acceptable
controller
{% endblock pod_not_backed_by_controller_failure_remediation %}

{% block pod_not_backed_by_controller_success_reason %}
No "no.scale.down.node.pod.not.backed.by.controller" errors found between {start_time} and {end_time}
{% endblock pod_not_backed_by_controller_success_reason %}


{% block not_safe_to_evict_annotation_failure_reason %}
The scaleDown event failed because a Pod on the node has the safe-to-evict=false annotation
Example log entry that would help identify involved objects:

{log_entry}
{% endblock not_safe_to_evict_annotation_failure_reason %}

{% block not_safe_to_evict_annotation_failure_remediation %}
If the Pod can be safely evicted, edit the manifest of the Pod and update the annotation to
"cluster-autoscaler.kubernetes.io/safe-to-evict": "true".
{% endblock not_safe_to_evict_annotation_failure_remediation %}

{% block not_safe_to_evict_annotation_success_reason %}
No "no.scale.down.node.pod.not.safe.to.evict.annotation" errors found between {start_time} and {end_time}
{% endblock not_safe_to_evict_annotation_success_reason %}


{% block pod_kube_system_unmovable_failure_reason %}
The scaleDown event failed because the pod is a non-DaemonSet, non-mirrored, Pod without a PodDisruptionBudget in the
kube-system namespace.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock pod_kube_system_unmovable_failure_reason %}

{% block pod_kube_system_unmovable_failure_remediation %}
By default, Pods in the kube-system namespace aren't removed by cluster autoscaler.

To resolve this issue, either add a PodDisruptionBudget for the kube-system Pods or use a combination of node pools
taints and tolerations to separate kube-system Pods from your application Pods.
To learn more, see
https://cloud.google.com/kubernetes-engine/docs/troubleshooting/cluster-autoscaler-scale-down#kube-system-unmoveable
{% endblock pod_kube_system_unmovable_failure_remediation %}

{% block pod_kube_system_unmovable_success_reason %}
No "no.scale.down.node.pod.kube.system.unmovable" errors found between {start_time} and {end_time}
{% endblock pod_kube_system_unmovable_success_reason %}


{% block pod_not_enough_pdb_failure_reason %}
The scaleDown event failed the pod doesn't have enough PodDisruptionBudget.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock pod_not_enough_pdb_failure_reason %}

{% block pod_not_enough_pdb_failure_remediation %}
Review the PodDisruptionBudget for the Pod and consider making it less restrictive.
To learn more, see
https://cloud.google.com/kubernetes-engine/docs/troubleshooting/cluster-autoscaler-scale-down#not-enough-pdb
{% endblock pod_not_enough_pdb_failure_remediation %}

{% block pod_not_enough_pdb_success_reason %}
No "no.scale.down.node.pod.not.enough.pdb" errors found between {start_time} and {end_time}
{% endblock pod_not_enough_pdb_success_reason %}


{% block pod_controller_not_found_failure_reason %}
Pod is blocking the ScaleDown event because its controller (for example, a Deployment or ReplicaSet) can't be found.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock pod_controller_not_found_failure_reason %}

{% block pod_controller_not_found_failure_remediation %}
To determine what actions were taken that left the Pod running after its controller was removed, review the logs. To
resolve this issue, manually delete the Pod.
{% endblock pod_controller_not_found_failure_remediation %}

{% block pod_controller_not_found_success_reason %}
No "no.scale.down.node.pod.controller.not.found" errors found between {start_time} and {end_time}
{% endblock pod_controller_not_found_success_reason %}


{% block pod_unexpected_error_failure_reason %}
Pod is blocking the ScaleDown event because of an unexpected error.
Example log entry that would help identify involved objects:

{log_entry}
{% endblock pod_unexpected_error_failure_reason %}

{% block pod_unexpected_error_failure_remediation %}
The root cause of this error is unknown. Contact Cloud Customer Care for further investigation.
{% endblock pod_unexpected_error_failure_remediation %}

{% block pod_unexpected_error_success_reason %}
No "no.scale.down.node.pod.unexpected.error" errors found between {start_time} and {end_time}
{% endblock pod_unexpected_error_success_reason %}
