dataproc_cluster_name=job_failed,job_id=1234567891,project_id=gcpdiag-dataproc1-aaaa,region=us-
central1

dataproc/spark-job-failures: Provides a comprehensive analysis of common issues which affects Dataproc Spark job failures.

  This runbook focuses on a range of potential problems for Dataproc Spark jobs
  on
  Google Cloud Platform. By conducting a series of checks, the runbook aims to
  pinpoint the root cause of Spark job failures.

  The following areas are examined:

  - Cluster version supportability: Evaluates if the job was run on a supported
  cluster image version.
  - Permissions: Checks for permission related issues on the cluster and GCS
  bucket level.
  - OOM: Checks Out-Of-Memory issues for the Spark job on master or worker
  nodes.
  - Logs: Check other logs related to shuffle failures, broken pipe, YARN
  runtime exception, import failures.
  - Throttling: Checks if the job was throttled and provides the exact reason
  for it.
  - GCS Connector: Evaluates possible issues with the GCS Connector.
  - BigQuery Connector: Evaluates possible issues with BigQuery Connector, such
  as dependency version conflicts.
  
[START]: Verify job exists in customer's project.
[INFO]: Start time utc:2024-11-01 03:12:35.635169+00:00
[INFO]: End time utc:2024-11-08 03:12:35.635169+00:00
[GATEWAY]: Execute child steps depending on if the required details exist or not
[COMPOSITE STEP]: Verify if job didn't failed with 'task not found' error.

   - gcpdiag-dataproc1-aaaa/1234567891/us-central1                        [OK]
     [REASON]
     Job `1234567891` did not fail due to a 'task not found' error. Unable to find the cluster deletion log between 2024-11-01 03:12:35.635169+00:00 and 2024-11-08 03:12:35.635169+00:00. It could be some other issue.Please raise a support case to investigate further.

[AUTOMATED STEP]: Verify if OOM has happened on master .

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     Didn't find logs messages related to Master OOM on the cluster: job-failed.

[AUTOMATED STEP]: Verify if OOM has happened on worker nodes.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     Didn't find logs messages related to Worker OOM on the cluster: job-failed.

[COMPOSITE STEP]: Check if secondary worker preemption has happened.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     Didn't find logs messages related to secondary worker preemption on the cluster: job-failed.

[COMPOSITE STEP]: Check if secondary worker preemption has happened.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     NOTICE: No message available to parse for this step
[COMPOSITE STEP]: Verify if the port exhaustion has happened.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Address already in use: Service 'sparkDriver' failed after 1000 retries" were found on the cluster: job-failed.

[COMPOSITE STEP]: Verify if the killing of Orphaned applications has happened.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Killing orphaned yarn application" were found on the cluster: job-failed.

[COMPOSITE STEP]: Check Python import failure.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "ImportError: cannot import name" were found on the cluster: job-failed.

[COMPOSITE STEP]: Check Shuffle Service Kill logs and autoscaling & preemptibility.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No shuffle service failure detected in cluster job-failed

[AUTOMATED STEP]: Checking autoscaling policies and graceful decommission timeouts.
[COMPOSITE STEP]: Check if STW GC Pause has happened on the cluster.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Detected pause in JVM or host machine (eg GC)" were found on the cluster: job-failed.

[COMPOSITE STEP]: Check for CheckYarnRuntimeException logs.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "YarnRuntimeException: Could not load history file .* /mapreduce-job-history/intermediate-done/root" were found on the cluster: job-failed.

[COMPOSITE STEP]: Check for Job Throttling messages in the logs.
[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Too many running jobs" were found on the cluster: job-failed.

[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Not enough free memory" were found on the cluster: job-failed.

[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "High system memory usage" were found on the cluster: job-failed.

[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Rate limit" were found on the cluster: job-failed.

[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Master agent not initialized" were found on the cluster: job-failed.

[AUTOMATED STEP]: Check if investigated logs messages exist in the Dataproc cluster.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No log messages related to "Disk space too low on Master" were found on the cluster: job-failed.

[COMPOSITE STEP]: Check for non-default GCS connector.
[AUTOMATED STEP]: Check for logs indicating shuffle failures.

   - gcpdiag-dataproc1-aaaa                                               [OK]
     [REASON]
     No shuffle failure logs found for cluster job-failed

[END]: This is the end step of the runbook.
[INFO]: Please visit all the FAIL steps and address the suggested remediations.
        If the REMEDIATION suggestions were not able to solve your issue please open a Support case
        with failed job details:
          1. Driver output
          2. YARN application logs
          3. (optional) Event logs, if you are facing a performance issue
          4. (optional) If there was a successful run in the past,
          provide job id and logs of that run


dataproc_cluster_name=job-not-failed,job_id=1234567890,project_id=gcpdiag-dataproc1-aaaa,region=us-
central1

dataproc/spark-job-failures: Provides a comprehensive analysis of common issues which affects Dataproc Spark job failures.

  This runbook focuses on a range of potential problems for Dataproc Spark jobs
  on
  Google Cloud Platform. By conducting a series of checks, the runbook aims to
  pinpoint the root cause of Spark job failures.

  The following areas are examined:

  - Cluster version supportability: Evaluates if the job was run on a supported
  cluster image version.
  - Permissions: Checks for permission related issues on the cluster and GCS
  bucket level.
  - OOM: Checks Out-Of-Memory issues for the Spark job on master or worker
  nodes.
  - Logs: Check other logs related to shuffle failures, broken pipe, YARN
  runtime exception, import failures.
  - Throttling: Checks if the job was throttled and provides the exact reason
  for it.
  - GCS Connector: Evaluates possible issues with the GCS Connector.
  - BigQuery Connector: Evaluates possible issues with BigQuery Connector, such
  as dependency version conflicts.
  
[START]: Verify job exists in customer's project.

   - gcpdiag-dataproc1-aaaa                                               [SKIP]
     [REASON]
     Job 1234567890 completed successfully.If the job experienced slow performance, potential causesinclude data skew, changes in data volume, or network latency.If performance issues persist, open a support case and share theSpark event log for both the fast and slow job runs.


