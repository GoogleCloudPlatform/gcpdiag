{% block cluster_name_exists_success_reason %}
Cluster {cluster_name} exists in project {project_id}
{% endblock cluster_name_exists_success_reason %}

{% block cluster_name_exists_failure_reason %}
Cluster {cluster_name} does not exist in project {project_id}
{% endblock cluster_name_exists_failure_reason %}

{% block cluster_name_exists_failure_remediation %}
Create the cluster again and keep it in the ERROR state in the Dataproc UI, or manually provide additional parameters using the gcpdiag command:

`gcpdiag runbook dataproc/cluster-creation -p cluster_name=CLUSTER_NAME -p cluster_uuid=CLUSTER_UUID -p network=NETWORK_URI -p subnetwork=SUBNETWORK_URI -p service_account=SERVICE_ACCOUNT -p internal_ip_only=True/False --project=PROJECT_ID`

Refer to <https://gcpdiag.dev/runbook/diagnostic-trees/dataproc/> for guidance on specifying additional parameters.
{% endblock cluster_name_exists_failure_remediation %}

{% block stackdriver_success_reason %}
Stackdriver: Enabled
{% endblock stackdriver_success_reason %}

{% block stackdriver_uncertain_reason %}
Could not determine if the `dataproc:dataproc.logging.stackdriver.enable` property is enabled for cluster, possibly because the cluster was deleted. Subsequent checks requiring Cloud logging might be affected.
{% endblock stackdriver_uncertain_reason %}

{% block stackdriver_uncertain_remediation %}
Enable Cloud logging by creating a cluster with property dataproc:dataproc.logging.stackdriver.enable = true.
Refer to the guide for more details:
<https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties>
{% endblock stackdriver_uncertain_remediation %}

{% block unspported_image_version_success_reason %}
The cluster {cluster_name} is using a supported Dataproc image version.
{% endblock unspported_image_version_success_reason %}

{% block unspported_image_version_failure_reason %}
The cluster {cluster_name} is using an unsupported Dataproc image version. Run the job on a supported image version.
{% endblock unspported_image_version_failure_reason %}

{% block unspported_image_version_failure_remediation %}
Find supported Dataproc image versions in this document[1].
[1] <https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters>
{% endblock unspported_image_version_failure_remediation %}

{% block gcs_connector_success_reason %}
No user-specified Cloud Storage connector version was identified. The cluster is using the default version.
{% endblock gcs_connector_success_reason %}

{% block gcs_connector_uncertain_reason %}
A user-specified Cloud Storage connector version was identified for cluster. Using a non-default connector version can lead to issues if not required by the application, as Dataproc clusters include a default pre-installed GCS connector.
<https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported-dataproc-image-versions>
{% endblock gcs_connector_uncertain_reason %}

{% block gcs_connector_uncertain_remediation %}
Verify the setup is correct if using a non-default Cloud Storage connector by following:
<https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#non-default_connector_versions>
<https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#service_account_permissions>
{% endblock gcs_connector_uncertain_remediation %}

{% block bq_connector_success_reason %}
The cluster uses image version {image_version} which preinstalls the BigQuery connector, and no conflicting BigQuery JARs were provided. Dependency version conflicts on the BigQuery side are not expected.

Refer to the Dataproc Version page to find out each component version preinstalled on your cluster:
<https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters>
{% endblock bq_connector_success_reason %}

{% block bq_connector_uncertain_reason %}
The cluster uses image version {image_version} which preinstalls the BigQuery connector, and a different version of the BigQuery connector is being installed at the cluster or job level. This might cause dependency version conflicts and lead to job failures.
{% endblock bq_connector_uncertain_reason %}

{% block bq_connector_uncertain_remediation %}
Resolve potential BigQuery connector version conflicts using one of the following approaches:

- If providing the BigQuery JAR at the cluster level: Create the Dataproc cluster without specifying any BigQuery JAR.
- If providing the BigQuery JAR at the job level: Run the job without specifying any BigQuery JAR.
- If installing a BigQuery JAR is necessary: Match the version of the BigQuery JAR to the version preinstalled on the cluster (version {bq_version} for image {image_version}).

Refer to the Dataproc Version page to find out each component version preinstalled on your cluster:
<https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters>
{% endblock bq_connector_uncertain_remediation %}
