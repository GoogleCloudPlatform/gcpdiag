{% block confirmation_success_reason %}
All backends are healthy in backend service {name} in scope {region}.
{% endblock confirmation_success_reason %}

{% block confirmation_failure_reason %}
The backend service {name} in the {region} scope has unhealthy backends.

{detailed_reason}
The backend service {name} uses the following health check: {hc_name}.

{success_criteria}

{timing_and_threshold}
{% endblock confirmation_failure_reason %}

{% block logging_enabled_success_reason %}
Health check logging is enabled for health check {hc_url}.
{% endblock logging_enabled_success_reason %}

{% block logging_enabled_uncertain_reason %}
Logging is not enabled for health check {hc_url}. Lack of logs hinders troubleshooting, as logs provide visibility into health check state changes and probe details needed to diagnose failures.
{% endblock logging_enabled_uncertain_reason %}

{% block logging_enabled_uncertain_remediation %}
Enable logging for the health check using the following `gcloud` command:

```
gcloud compute health-checks update {protocol} {hc_name} {additional_flags} --enable-logging
```

This will log any future changes in health status, but won't show past activity. Note that new health check logs will only appear when a health state transition occurs.
{% endblock logging_enabled_uncertain_remediation %}

{% block port_mismatch_uncertain_reason %}
The load balancer is conducting health checks on port {hc_port} for the backend service {bs_resource}. However, this health check port differs from the port used by the load balancer for serving traffic on some backend instance groups. The backend service is configured to use the "{serving_port_name}" port, which is then translated to a specific port number based on the "{serving_port_name}" port mapping within each backend instance group.

Affected backends:

{formatted_igs}

This configuration can be problematic unless the load balancer has been configured to use a different port for health checks purposefully.
{% endblock port_mismatch_uncertain_reason %}

{% block port_mismatch_uncertain_remediation %}
1.  **Verify Intent:** Confirm if the health check port `{hc_port}` is *meant* to be different from the serving port defined by "{serving_port_name}" on the backends.

2.  **Test Port on Backend VMs:** Check if port `{hc_port}` is listening on an instance from the affected groups. Run this command from your local machine/Cloud Shell:

    ```bash
    gcloud compute ssh [INSTANCE_NAME] --zone [ZONE] --project {project_id} --command="sudo ss -tlnp | grep ':{hc_port}'"
    ```
    *   Output showing `LISTEN` indicates the port is open and your application is likely listening.
    *   No output suggests the port is not in a listening state on that VM.

3.  **Adjust Configuration:**
    *   **If Mismatch is Unintentional:** Align the load balancer's health check port in the backend service `{bs_resource}` to match the actual port number used by "{serving_port_name}" in the instance groups.
    *   **If Mismatch is Intentional:** Ensure your application on the VMs is correctly configured to listen on port `{hc_port}`.
    *   **If Port Not Listening:** Troubleshoot your application on the VM to ensure it's running and bound to port `{hc_port}`. Check the VM's local firewall as well.

If the health check port `{hc_port}` is meant to be different from the serving port (e.g., a dedicated management/health endpoint), confirm that your application is correctly configured to listen on the health check port.
{% endblock port_mismatch_uncertain_remediation %}

{% block port_mismatch_uncertain_reason_a1 %}
Backend service "{backend_service_name}" is a {lb_scheme} Passthrough Load Balancer. These load balancers forward traffic to backends without changing the destination port, so there isn't a single "serving port" on the load balancer itself. The health check probes are sent to the port defined directly in the HealthCheck resource. The associated health check "{hc_name}" is configured to use port {hc_port}.
{% endblock port_mismatch_uncertain_reason_a1 %}

{% block port_mismatch_uncertain_remediation_a1 %}
Please ensure that the backend instances (VMs or endpoints) are configured to accept and respond to health check probes specifically on port {hc_port}. The application running on the backends must be actively listening on this port ({hc_port}) for the health checks to succeed, regardless of the ports used for actual application traffic.
{% endblock port_mismatch_uncertain_remediation_a1 %}

{% block port_mismatch_success_reason %}
The load balancer is performing health checks on the same port used for serving traffic. This is the standard configuration.

{port_mapping}
{% endblock port_mismatch_success_reason %}

{% block protocol_mismatch_uncertain_reason %}
The load balancer uses {serving_protocol} for traffic but {hc_protocol} for health checks on backend service {bs_resource}. If not intended, this protocol mismatch can lead to incorrect health assessments, potentially causing traffic to be sent to failing backends or triggering unnecessary failovers.

Here are some examples of potentially problematic mismatches:

*   **TLS vs. Non-TLS:**
    *   Health Check: HTTPS, Backend: HTTP - The health check will try to initiate a TLS handshake, which will fail against a non-TLS server.
    *   Health Check: HTTP, Backend: HTTPS - The health check sends plaintext, but the server expects TLS, likely resulting in a connection reset or protocol error.

*   **Application Protocol Mismatch:**
    *   Health Check: GRPC, Backend: HTTP - The health check speaks the GRPC protocol, but the backend expects standard HTTP requests.
    *   Health Check: HTTP, Backend: SSL - The health check expects an HTTP application response, but the backend is configured for generic SSL, which might not involve HTTP.

*   **Protocol Version/Feature Mismatch (Subtler issues even with the same base protocol):**
    *   An HTTP/1.0 health check request to a server strictly requiring HTTP/1.1 features.
    *   An HTTP/2 health check to a server only supporting HTTP/1.1 without proper negotiation.

**Important:** Health checks using {hc_protocol} might be passing while the application serving {serving_protocol} traffic is failing because the success criteria for the two protocols can differ. More details on the health check success criteria can be found in [docs](https://cloud.google.com/load-balancing/docs/health-check-concepts#criteria-protocol-http).
{% endblock protocol_mismatch_uncertain_reason %}

{% block protocol_mismatch_uncertain_remediation %}
1.  Verify if this protocol difference is intentional and well-understood.
2.  If not, **align the health check protocol with the serving protocol ({serving_protocol})** to ensure health checks accurately represent the backend's ability to serve traffic.
3.  Consult the [Health Checks Overview](https://cloud.google.com/load-balancing/docs/health-check-concepts) for best practices.
{% endblock protocol_mismatch_uncertain_remediation %}

{% block protocol_mismatch_success_reason %}
The load balancer is performing health checks using the same protocol ({hc_protocol}) used for serving traffic on backend service {bs_resource}. This is the standard configuration.
{% endblock protocol_mismatch_success_reason %}

{% block protocol_mismatch_success_reason_a1 %}
The backend service {bs_resource} uses {serving_protocol} for traffic and a TCP health check. TCP health checks are broadly compatible as they only test basic port connectivity, not application-level responses. This is a common configuration.
{% endblock protocol_mismatch_success_reason_a1 %}

{% block firewall_rules_failure_reason %}
{insight}
The health checks are currently failing due to a misconfigured firewall. This prevents Google Cloud probers from connecting to the backends, causing the load balancer to consider them unhealthy.
{% endblock firewall_rules_failure_reason %}

{% block firewall_rules_failure_remediation %}
Update the firewall rules to allow inbound traffic from the Google Cloud health check IP ranges (found at <https://cloud.google.com/load-balancing/docs/health-check-concepts#ip-ranges>) to the backends.
{% endblock firewall_rules_failure_remediation %}

{% block firewall_rules_success_reason %}
Firewall rules are correctly configured and are not blocking health check probes for backend service {bs_url}.
{% endblock firewall_rules_success_reason %}

{% block past_hc_success_uncertain_remediation %}
Check the logs and monitoring metrics for the instances associated with backend service {bs_url}, focusing on recent timeframes to see if there were any errors, crashes, or resource exhaustion issues. Also inspect any application-specific logs for errors or warnings.
{% endblock past_hc_success_uncertain_remediation %}

{% block unknown_hc_state_log_failure_reason %}
Health check logs for backend service {bs_url} show entries with the detailed health state UNKNOWN. This indicates that the health checking system is aware of the instance, but its health status is undetermined. This situation can arise when a new endpoint is unresponsive to health checks and there's a substantial configured timeout period (approximately 25 seconds or longer). In such cases, the "UNKNOWN" state might be published while the health checker waits for the timeout to expire. Additionally, "UNKNOWN" could also be published during outage scenarios if the health checkers themselves are crashing. In this critical situation, endpoints that previously had known health states could transition to "UNKNOWN".
{% endblock unknown_hc_state_log_failure_reason %}

{% block unknown_hc_state_log_failure_remediation %}
For new endpoints: Consider reducing the timeout period for health checks if appropriate, especially during initial setup or testing phases.

For potential Google Cloud outages: Use Personalized Service Health to check for any ongoing incidents that might be affecting the project or the specific service in question. If an incident is identified, follow any recommended mitigation steps or wait for the issue to be resolved by Google Cloud.
{% endblock unknown_hc_state_log_failure_remediation %}

{% block unhealthy_hc_state_log_failure_reason %}
Health check logs for backend service {bs_url} indicate a detailed health state of UNHEALTHY. The backend instances are reachable but are not passing the health check requirements.

Responses received from backends: {probe_results_text_str}
{% endblock unhealthy_hc_state_log_failure_reason %}

{% block unhealthy_hc_state_log_failure_remediation %}
{success_criteria}

Investigate the configuration of the application to ensure it aligns with these health check expectations.

If a different endpoint should be checked or a different response is expected, adjust the health check settings accordingly.
{% endblock unhealthy_hc_state_log_failure_remediation %}

{% block timeout_hc_state_log_failure_reason %}
Health check logs for backend service {bs_url} show the detailed health state "TIMEOUT".

Responses received from backends: {probe_results_text_str}

The backend might be timing out because:

1. The application is overloaded and taking too long to respond.

2. The backend service or health check timeout is too low.

3. Connection to the endpoint cannot be established - the backend instance has crashed or is otherwise unresponsive.

The following responses were received from your backends: {probe_results_text_str}
{% endblock timeout_hc_state_log_failure_reason %}

{% block timeout_hc_state_log_failure_remediation %}

1. Make sure that the backend service timeout (current value: {bs_timeout_sec}s) and health check timeout (current value: {hc_timeout_sec}s) are appropriately configured to accommodate the application's expected response time.

2. Investigate the application's configuration to ensure it is correctly handling health check probe requests. {success_criteria}

3. Check if firewall rules or iptables configurations are blocking the health check probes from reaching the backend instances, resulting in timeouts.
{% endblock timeout_hc_state_log_failure_remediation %}
