{% block high_cpu_utilization_step_name %}
Verify VM CPU utilization is within optimal levels.
{% endblock high_cpu_utilization_step_name %}

{% block high_cpu_utilization_failure_reason %}
CPU utilization on this VM has surpassed recommended operational levels,
which may affect its performance and SSH connectivity.
{% endblock high_cpu_utilization_failure_reason %}

{% block high_cpu_utilization_failure_remediation %}
Excessive CPU usage can lead to performance bottlenecks. Consider resizing the VM to a more
powerful machine type with higher CPU capabilities.
Detailed instructions for resizing and restarting VMs are available here:
- Stopping a VM: https://cloud.google.com/compute/docs/instances/stop-start-instance
- Resizing a VM: https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud
Additionally, use the Compute Engine observability metrics for an in-depth analysis to pinpoint high-usage processes:
https://cloud.google.com/compute/docs/instances/observe-monitor-vms#access_vm_observability_metrics
https://cloud.google.com/compute/docs/instances/observe-monitor-vms#process_utilization

Alternatively, you can connect via serial console if SSH is unvailable to stop offending processes
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console.
{% endblock high_cpu_utilization_failure_remediation %}

{% block high_cpu_utilization_success_reason %}
The Compute Engine VM {full_resource_path},
has CPU utilization within the optimal range.
{% endblock high_cpu_utilization_success_reason %}

{% block high_disk_utilization_failure_reason %}
Disk utilization on this VM's boot disk is critically high,
potentially affecting application performance.
{% endblock high_disk_utilization_failure_reason %}

{% block high_disk_utilization_skipped_reason %}
No Google Cloud Ops Agent installed on the VM, making it difficult to retrieve disk utilization data via metrics.
Falling back to checking for filesystem utilization-related messages in the serial logs.
{% endblock high_disk_utilization_skipped_reason %}

{% block high_disk_utilization_failure_remediation %}
To mitigate high disk usage, consider expanding the VM's boot disk capacity.
This action can help avoid performance issues and ensure smoother SSH connections.
Follow the guide to increase disk size:
https://cloud.google.com/compute/docs/disks/resize-persistent-disk#increase_the_size_of_a_disk
{% endblock high_disk_utilization_failure_remediation %}

{% block high_disk_utilization_step_name %}
Check if VM's boot disk space utilization is within optimal levels.
{% endblock high_disk_utilization_step_name %}

{% block high_disk_utilization_success_reason %}
The boot disk space usage for the Compute Engine VM {full_resource_path}, is within optimal levels.
{% endblock high_disk_utilization_success_reason %}

{% block high_memory_utilization_skipped_reason %}
There are no logs to examine!
{% endblock high_memory_utilization_skipped_reason %}

{% block high_memory_utilization_failure_reason %}
Memory utilization on this VM has reached levels that may compromise its VM application performance.
{% endblock high_memory_utilization_failure_reason %}

{% block high_memory_utilization_failure_remediation %}
Elevated memory usage can result in slow or unresponsive or termimated applications.
Consider enhancing the VM's memory capacity by changing to a machine type with more memory.
Guidance on stopping and changing the machine type can be found here:
- Changing machine type:
https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud
For deeper analysis of memory issues:

Additionally, use the Compute Engine observability metrics for an in-depth analysis to pinpoint high-usage processes:
https://cloud.google.com/compute/docs/instances/observe-monitor-vms#memory_utilization

Or connect via the Serial Console if SSH is not available to mitigate the issue.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console
{% endblock high_memory_utilization_failure_remediation %}

{% block high_memory_utilization_success_reason %}
Memory utilization on this VM is within optimal range.
{% endblock high_memory_utilization_success_reason %}

{% block high_memory_usage_logs_skipped_reason %}
There are no serial logs to investigate possible memory issues.
{% endblock high_memory_usage_logs_skipped_reason %}

{% block high_memory_usage_logs_step_name %}
Check for memory related errors in VM serial logs.
{% endblock high_memory_usage_logs_step_name %}

{% block high_memory_usage_logs_success_reason %}
No memory-related errors found in Serial console logs.
{% endblock high_memory_usage_logs_success_reason %}

{% block high_memory_usage_logs_failure_reason %}
High memory utilization error logs have been detected on your VM instance within
{start_time} - {end_time}. This can lead to performance issues and instability.
{% endblock high_memory_usage_logs_failure_reason %}

{% block high_memory_usage_logs_failure_remediation %}
1. Manually investigate by accessing the Guest OS to Analyze Memory Usage:
* Identify processes with consistently high memory consumption:
- Run `top` and press "M" to sort processes by memory usage.
- Alternatively, use the command:
```bash
ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -n 5
```
* Focus on processes with recent spikes or consistently high memory usage.
* If SSH access is unavailable, consider [troubleshooting via the serial
console](https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console).

2. Review Application or Configuration Changes:
* Investigate if recent deployments, updates, or configuration changes correlate with increased memory usage.

3. Resolve Identified Bottlenecks:
* For applications causing excessive memory usage:
- Optimize their configuration or update them to a stable version.
- Explore alternatives if optimization is insufficient.
* Evaluate scaling up resources if the high memory usage results from legitimate application demands.

4. Increase Instance Memory if Necessary:
* If additional memory is required for the application:
- Stop the VM and [change its machine
type](https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud).
- Consult the [machine type documentation](https://cloud.google.com/compute/docs/machine-types) to select an appropriate
configuration.

**Note:**
Non Google provided Application-specific issues may fall outside support scope. Collaborate with
relevant application teams for further investigation.
Refer to the [Google Cloud Platform support
policy](https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope)
for details, including [out-of-scope
items](https://cloud.google.com/compute/docs/images/support-maintenance-policy#out-of-scope_for_support).
{% endblock high_memory_usage_logs_failure_remediation %}

{% block high_memory_usage_logs_uncertain_reason %}
Unable to investigate the high memory utilization error logs, likely due to the absence of logs.
However, this does not eliminate the possibility of high memory usage.

You may manually verify memory utilization on the Guest OS side a likely cause.
{% endblock high_memory_usage_logs_uncertain_reason %}


{% block high_disk_utilization_error_failure_reason %}
Found high disk utilisation errors in Serial console logs.
The patterns used:
'No space left on device', 'No usable temporary directory found',
'A stop job is running for Security \.\.\..* Service ',
'disk is at or near capacity'
{% endblock high_disk_utilization_error_failure_reason %}

{% block high_disk_utilization_error_failure_remediation %}
To mitigate high disk usage, consider expanding the VM's boot disk capacity.
This action can help avoid performance issues and ensure accessibility of the VM.
Follow the guide to increase disk size:
https://cloud.google.com/compute/docs/disks/resize-persistent-disk#increase_the_size_of_a_disk
{% endblock high_disk_utilization_error_failure_remediation %}

{% block high_disk_utilization_error_step_name %}
Checking for high disk utilization related logs in serial console logs
{% endblock high_disk_utilization_error_step_name %}

{% block high_disk_utilization_error_success_reason %}
No high disk utilisation related errors found in Serial console logs.
{% endblock high_disk_utilization_error_success_reason %}

{% block high_disk_utilization_error_skipped_reason %}
There are no logs to examine!
{% endblock high_disk_utilization_error_skipped_reason %}

{% block high_disk_utilization_error_uncertain_reason %}
No Serial console logs to examine
{% endblock high_disk_utilization_error_uncertain_reason %}

{% block live_migrations_failure_reason %}
Live migrations detected for the VM during mentioned period.
{% endblock live_migrations_failure_reason %}

{% block live_migrations_failure_remediation %}
As remediation, you may try to simulate the migration (move the VM to another host) using
https://cloud.google.com/compute/docs/instances/simulating-host-maintenance?hl=en#testingpolicies ,
and see if issue still persists. If yes, please reach out to Google Cloud Platform Support teams via case.

Note: During live migration, VMs might experience a decrease in performance in disk,
CPU, memory, and network utilization for a short period of time.

https://cloud.google.com/compute/docs/instances/live-migration-process#how_does_the_live_migration_process_work

{% endblock live_migrations_failure_remediation %}

{% block live_migrations_step_name %}
Verify live migrations happened for the instance
{% endblock live_migrations_step_name %}

{% block live_migrations_success_reason %}
No live migrations detected for the VM during mentioned period
{% endblock live_migrations_success_reason %}

{% block live_migrations_skipped_reason %}
There are no logs to examine !
{% endblock live_migrations_skipped_reason %}


{% block slow_disk_io_step_name %}
Verify any slow Disk operations related errors in Serial console logs
{% endblock slow_disk_io_step_name %}

{% block slow_disk_io_failure_reason %}
Possibe Disk IO slowness detected.

The patterns used:
# Linux slow read:
r'\d+:\d+:\d+:\d+: timing out command, waited \d+s',
r'end_request: I/O error, dev [a-z0-9-]+, sector \d+',
r'Buffer I/O error on device [a-z0-9-]+, logical block \d+',
r'blocked for more than \d+ seconds',

# Linux SCSI commands abort/reset (when operation to PD times out)
r'\d+:\d+:\d+:\d+:\s+\[([a-z0-9-]+)\]\s+(abort|device reset)$',
r'\d+:\d+:\d+:\d+:\s+(device reset)$',

# Linux Local SSD physical failure on console:
r'kernel: blk_update_request: I/O error, dev [a-z0-9-]+, sector \d+',

# Windows
r'The IO operation at logical block address 0x[0-9a-fA-F.]+ for Disk \d+ \(PDO name: \\Device\\.*\) was retried'
{% endblock slow_disk_io_failure_reason %}

{% block slow_disk_io_failure_remediation %}
There can be multiple reasons which can cause Slow Disk IOs:

- CPU Starvation - Small instances(with lesser CPUs) may not have enough CPU to serve all I/Os inflight.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#cpu_and_memory_performance

- Network Throttling - High sent/received network traffic can cause network throttling that impacts disk operations.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#network_performance

- Insufficient Machine Resources - If your machine's IOPS and throughput limts are not enought to serve your workloads,
this can also cause CPU or Disk IOPS/throughput Starvation.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#storage_performance

- Application and GuestOS Operations - Unmanaged and untested application workloads can cause the high influx of IOs
to the disk and cause IO operations to be queued, causing throttling at disk and CPU levels.

To fix this issue:
- Please optmize your application workloads.
- If needed, please add more resources(CPU, Memory) to the VM.
- Please optmize your Disk performance -
https://cloud.google.com/compute/docs/disks/optimizing-pd-performance
- If needed, please change your disk type to get better Disk IOPS/throughput limits -
https://cloud.google.com/compute/docs/disks/modify-persistent-disk#disk_type

{% endblock slow_disk_io_failure_remediation %}

{% block slow_disk_io_skipped_reason %}
There are no logs to examine !
{% endblock slow_disk_io_skipped_reason %}

{% block slow_disk_io_uncertain_reason %}
No error messages related to disk latency were found in the serial console logs.
This does not rule out disk performance issues.
{% endblock slow_disk_io_uncertain_reason %}

{% block slow_disk_io_uncertain_remediation %}
Check for high disk utilization using the command iostat to further troubleshoot.
{% endblock slow_disk_io_uncertain_remediation %}

{% block disk_io_usage_check_step_name %}
Verify the Disk IOPS/Throughput usage is within optimal limits
{% endblock disk_io_usage_check_step_name %}

{% block disk_io_usage_check_failure_reason %}
Disk IOPS/Throughput usage is NOT within optimal limits
{% endblock disk_io_usage_check_failure_reason %}

{% block disk_io_usage_check_failure_remediation %}
There can be multiple reasons which can cause Disk IOPS/Throughput usage to increase:

- Application and GuestOS Operations - Unmanaged and untested application workloads can cause the high influx of IOs
to the disk and cause IO operations to be queued, causing throttling at disk levels.

- CPU Starvation - Small instances(with lesser CPUs) may not have enough CPU to serve all I/Os inflight.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#cpu_and_memory_performance

- Network Throttling - High sent/received network traffic can cause network throttling, that can also impacts disk
operations.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#network_performance

- Insufficient Machine Resources - If your machine's IOPS and throughput limts are not enought to serve your workloads,
this can also cause CPU or Disk IOPS/throughput Starvation.
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#storage_performance

To fix this issue:
- Please optmize your application workloads.
- If needed, please add more resources(CPU, Memory) to the VM.
- Please optmize your Disk performance -
https://cloud.google.com/compute/docs/disks/optimizing-pd-performance
- If needed, please change your disk type to get better Disk IOPS/throughput limits -
https://cloud.google.com/compute/docs/disks/modify-persistent-disk#disk_type

{% endblock disk_io_usage_check_failure_remediation %}


{% block disk_health_check_step_name %}
Verify instance disks are healthy
{% endblock disk_health_check_step_name %}

{% block disk_health_check_success_reason %}
Instance disk "{disk_name}" is healthy.
{% endblock disk_health_check_success_reason %}

{% block disk_health_check_failure_reason %}
You might experience slower/poor performance with your disk '{disk_name}' due to an
ongoing issue with our Compute Engine or Persistent Disk infrastructure. We're working
to resolve this as quickly as possible.
{% endblock disk_health_check_failure_reason %}

{% block disk_health_check_failure_remediation %}
To better understand the situation with your Compute Engine or Persistent Disks,
could you please take a look at the Google Cloud Status page:

https://status.cloud.google.com

This page provides real-time updates on the health of Google Cloud services.

Additionally, it may be helpful to check the Service Health dashboard in your
Google Cloud Console for any reported incidents:

https://console.cloud.google.com/servicehealth/incidents

If you don't find any information about an ongoing issue related to your concern,
please don't hesitate to reach out to Google Cloud Support by creating a support case.
They'll be happy to investigate further and assist you.
{% endblock disk_health_check_failure_remediation %}


{% block disk_io_latency_check_step_name %}
Verify Instance's Disk Avg IO Latency is within optimal limits
{% endblock disk_io_latency_check_step_name %}

{% block disk_io_latency_check_success_reason %}
Instance disk "{disk_name}"'s IO latency is within the optimal limtis.
{% endblock disk_io_latency_check_success_reason %}

{% block disk_io_latency_check_failure_reason %}
The performance of the disk '{disk_name}' is currently degraded due to high
IO latency exceeding optimal thresholds. This may result in slower read/write
speeds and overall reduced performance.
{% endblock disk_io_latency_check_failure_reason %}

{% block disk_io_latency_check_failure_remediation %}
Disk I/O latency is the time it takes for a read or write operation to complete on a
disk.
High disk I/O latency can significantly impact the performance of your applications
and workloads running on the instance, leading to slow response times, increased
processing time, and overall sluggishness.

Potential Bottlenecks -
- Disk Type: To optimize disk performance, ensure your disk type is appropriate
for your workload and provides acceptable latency for your system architecture.
Choosing the right disk type can significantly impact performance.
https://cloud.google.com/compute/docs/disks

- Workload: The nature of your workload also influences latency. Workloads with
many small, random I/O operations will generally have higher latency than those
with sequential I/O

Optimize Disk Usage:
- Reduce I/O Operations: Optimize your applications and database queries to minimize
the number of disk I/O operations.
- Increase I/O Request Size: Larger I/O requests can be more efficient than many small
ones. Consider adjusting your application or database settings to increase the I/O
request size.
- Caching: Implement caching mechanisms to reduce the need to access the disk for
frequently used data.

Choose the Right Disk Type with lesser IO Latency - https://cloud.google.com/compute/docs/disks

You may also look into Optimizing persistent disk performance. -
https://cloud.google.com/compute/docs/disks/optimizing-pd-performance

Please don't hesitate to reach out to Google Cloud Support if issue is not resolved.
{% endblock disk_io_latency_check_failure_remediation %}
