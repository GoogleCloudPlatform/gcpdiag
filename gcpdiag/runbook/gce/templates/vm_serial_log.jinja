{% block default_failure_reason %}
Anomalies detected in the serial logs which align with the investigated bad patterns
{% endblock default_failure_reason %}

{% block default_skipped_reason %}
There are no logs to examine.
{% endblock default_skipped_reason %}

{% block default_failure_remediation %}
Investigate potential issues through the serial console.
If GRUB_TIMEOUT is greater than 0, access the interactive session for more insights.
Explore rescue options for inaccessible VMs or review possible guest OS issues.

- Interactive Serial Console: <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>
- Rescuing VMs: <https://cloud.google.com/compute/docs/troubleshooting/rescue-vm>

If escalating Guest OS related issues to Google Cloud Support,
do check to ensure is in line with Google Cloud Platform's Guest OS support policy

- Google Cloud Platform Support Scope:
<https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope>
{% endblock default_failure_remediation %}

{% block default_success_reason %}
The VM's Linux OS shows no signs of interested anomalies,
indicating a *likely* stable operational state.
{% endblock default_success_reason %}

{% block default_uncertain_reason %}
Lack of serial log data prevented a thorough assessment of the VM's operational state. Result is
inconclusive
{% endblock default_uncertain_reason %}

{% block default_uncertain_remediation %}
Confirm the VM's operational status by reviewing available serial logs.
Address any detected guest OS issues using the provided documentation,
keeping in mind certain guest OS faults may be beyond Google Cloud Platform's support scope.

- Viewing Serial Port Output: <https://cloud.google.com/compute/docs/troubleshooting/viewing-serial-port-output>
- Resolving Kernel Panic:
<https://cloud.google.com/compute/docs/troubleshooting/kernel-panic#resolve_the_kernel_panic_error>
- Google Cloud Platform Support Scope:
<https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope>
{% endblock default_uncertain_remediation %}

{% block kernel_panic_step_name %}
Examine Guest OS if there are any indications of kernel panic.
{% endblock kernel_panic_step_name %}

{% block kernel_panic_success_reason %}
A review of the serial console logs for the GCE instance `{full_resource_path}` from `{start_time}` to `{end_time}`
shows no evidence of kernel panic or GRUB issues. The `systemd` application is confirmed to be running.
{% endblock kernel_panic_success_reason %}

{% block kernel_panic_failure_reason %}
Detected kernel panic logs in {full_resource_path} serial logs,
which is likely preventing the VM from booting up correctly.
{% endblock kernel_panic_failure_reason %}

{% block kernel_panic_failure_remediation %}
Kernel panics can be caused by different issues within the guest.
Address underlying issues causing boot problems to solve the kernel panic:

**General Kernel panic Troubleshooting**

1. Consult the Troubleshooting Guide for Kernel Panic Errors:
   - Kernel panic is commonly caused by file system errors in Linux Guest OS.
   - Check `/etc/fstab` for incorrect entries that could halt the boot process.
   - Refer to this guide for resolving [kernel panic issues caused by
/etc/fstab](https://cloud.google.com/compute/docs/troubleshooting/fstab-errors).

2. Resources for Kernel panic
   - [Troubleshooting GCE Instance experiencing Kernel
Panic](https://cloud.google.com/compute/docs/troubleshooting/kernel-panic#resolve_the_kernel_panic_error)
   - [Common Red Hat Kernel Panic
Issues](https://access.redhat.com/search/knowledgebase?q=kernel+panic&p=1&rows=10&documentKind=Solution%26Documentation&sort=relevant)
   - [Common SUSE Kernel Panic Issues](https://www.suse.com/support/kb/?id=&q=kernel+panic&bu_suse=true&advanced=false)

3. Rescue an instance experiencing kernel panic
   - [How to rescue a experiencing kernel panic](https://cloud.google.com/compute/docs/troubleshooting/rescue-vm) to
recover faulty VMs.
   - Watch this video for a walkthrough: [Rescue VM Guide](https://www.youtube.com/watch?v=oD6IFpjEtEw)

4 Google Cloud Platform Support Scope:
[Understand GCP support for kernel-related
issues](https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope).
{% endblock kernel_panic_failure_remediation %}

{% block kernel_panic_uncertain_reason %}
No serial logs were found for the VM {full_resource_path}. However, this does not rule out the possibility of a kernel
panic.
{% endblock kernel_panic_uncertain_reason %}

{% block kernel_panic_uncertain_remediation %}
Manually [view the most recent serial
logs](https://cloud.google.com/compute/docs/troubleshooting/viewing-serial-port-output)
to investigate issues with the Guest OS applications.

Kernel panics can be caused by different issues within the guest.
Address underlying issues causing boot problems to solve the kernel panic:

**General Kernel panic Troubleshooting**

1. Consult the Troubleshooting Guide for Kernel Panic Errors:
   - Kernel panic is commonly caused by file system errors in Linux Guest OS.
   - Check `/etc/fstab` for incorrect entries that could halt the boot process.
   - Refer to this guide for resolving [kernel panic issues caused by
/etc/fstab](https://cloud.google.com/compute/docs/troubleshooting/fstab-errors).

2. Resources for Kernel panic
   - [Troubleshooting GCE Instance experiencing Kernel
Panic](https://cloud.google.com/compute/docs/troubleshooting/kernel-panic#resolve_the_kernel_panic_error)
   - [Common Red Hat Kernel Panic
Issues](https://access.redhat.com/search/knowledgebase?q=kernel+panic&p=1&rows=10&documentKind=Solution%26Documentation&sort=relevant)
   - [Common SUSE Kernel Panic Issues](https://www.suse.com/support/kb/?id=&q=kernel+panic&bu_suse=true&advanced=false)

3. Rescue an instance experiencing kernel panic
   - [How to rescue a experiencing kernel panic](https://cloud.google.com/compute/docs/troubleshooting/rescue-vm) to
recover faulty VMs.
   - Watch this video for a walkthrough: [Rescue VM Guide](https://www.youtube.com/watch?v=oD6IFpjEtEw)

4 Google Cloud Platform Support Scope:
[Understand GCP support for kernel-related
issues](https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope).
{% endblock kernel_panic_uncertain_remediation %}

{% block kernel_panic_skipped_reason %}
There are no logs to examine.
{% endblock kernel_panic_skipped_reason %}

{% block sshguard_step_name %}
Verify if SSHGuard is installed and blocking SSH connectivity
{% endblock sshguard_step_name %}

{% block sshguard_failure_reason %}
SSHGuard is active and may be blocking IP addresses.
Verify if your IP is blocked by SSHGuard on the VM.
{% endblock sshguard_failure_reason %}

{% block sshguard_failure_remediation %}
Note: Issues related to SSHGuard fall outside the standard support scope for Google Cloud Platform.
Consult the most appropriate team within your organisation to assist with resolution.
For guest OS issues and SSHGuard configurations, refer to:

- Support Scope: <https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope>
- Out of Scope Support: <https://cloud.google.com/compute/docs/images/support-maintenance-policy#out-of-scope_for_support>
{% endblock sshguard_failure_remediation %}

{% block sshguard_success_reason %}
SSHGuard doesn't appear to be blockaging IPs on the VM (if installed on the VM)
{% endblock sshguard_success_reason %}

{% block sshguard_uncertain_reason %}
The retrieved logs do not contain definitive entries, either positive or negative,
to make a conclusive assessment.
Manually review the GCE serial logs to determine if SSHGuard is a likely cause.
{% endblock sshguard_uncertain_reason %}

{% block sshguard_uncertain_remediation %}
If SSHGuard or similar application is a concern,
manually inspect its configuration via the interactive serial console:
<https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>
{% endblock sshguard_uncertain_remediation %}

{% block sshguard_skipped_reason %}
There are no logs to examine
{% endblock sshguard_skipped_reason %}

{% block windows_gce_ssh_agent_instructions_message %}
Verify if the `google-compute-engine-ssh` agent is installed on the Windows VM.
{% endblock windows_gce_ssh_agent_instructions_message %}

{% block windows_gce_ssh_agent_step_name %}
Manually verify if the necessary Google guest agents, especially `google-compute-engine-ssh`,
are operational on the VM.
{% endblock windows_gce_ssh_agent_step_name %}

{% block windows_gce_ssh_agent_uncertain_reason %}
Uncertain if the `google-compute-engine-ssh` agent is installed on the VM.
{% endblock windows_gce_ssh_agent_uncertain_reason %}

{% block windows_gce_ssh_agent_uncertain_remediation %}
Utilize RDP or a startup script to check for the presence and functionality of the
`google-compute-engine-ssh` agent. For instructions on implementing startup scripts, refer to:
<https://cloud.google.com/compute/docs/connect/windows-ssh#startup-script>
{% endblock windows_gce_ssh_agent_uncertain_remediation %}

{% block windows_gce_ssh_agent_success_reason %}
Successfully confirmed the installation of the `google-compute-engine-ssh` agent on the Windows VM.
{% endblock windows_gce_ssh_agent_success_reason %}

{% block windows_gce_ssh_agent_failure_reason %}
Unable to confirm the installation of the `google-compute-engine-ssh` agent on the VM.
{% endblock windows_gce_ssh_agent_failure_reason %}

{% block windows_gce_ssh_agent_failure_remediation %}
Ensure the `google-compute-engine-ssh` agent is correctly installed and configured.
Consult the following guide for assistance with agent installation and configuration:
<https://cloud.google.com/compute/docs/connect/windows-ssh#startup-script>
{% endblock windows_gce_ssh_agent_failure_remediation %}

{% block sshd_step_name %}
Verify OpenSSH daemon (sshd) has started from most recent serial logs.
{% endblock sshd_step_name %}

{% block sshd_failure_reason %}
The latest OpenSSH daemon (sshd) logs indicate that the daemon has either failed to start or is misconfigured.
This issue is preventing proper SSH connectivity to the VM.
{% endblock sshd_failure_reason %}

{% block sshd_failure_remediation %}
Google Cloud Compute Engine provides regional serial console gateways for troubleshooting the
Guest OS when SSHD is unavailable.

Connect to the VM using the [interactive serial console
gateways](https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console#connectserialconsole).

- Troubleshooting Common Linux `sshd.service` Errors:
[Guide for resolving common SSH
errors](https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh-errors#linux_errors)
- Knowledge Base for SSHD Service Failures:
[Diagnosing SSH service
issues](https://cloud.google.com/knowledge/kb/ssh-in-cloud-serial-console-fails-with-warning-message-000004554)

Note: Guest OS issues are outside the scope of Google Cloud Platform support:
<https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope> and
<https://cloud.google.com/compute/docs/images/support-maintenance-policy#out-of-scope_for_support>
{% endblock sshd_failure_remediation %}

{% block sshd_success_reason %}
The latest OpenSSH daemon (sshd) logs indicate that the daemon has started.
{% endblock sshd_success_reason %}

{% block sshd_uncertain_reason %}
The serial logs for Compute Engine VM {full_resource_path} do not contain positive or negative entries
to conclusively assess if sshd.service is up and running correctly.
{% endblock sshd_uncertain_reason %}

{% block sshd_skipped_reason %}
There are no logs to examine
{% endblock sshd_skipped_reason %}

{% block windows_bootup_step_name %}
Verify Windows boot up process have successfully completed.
{% endblock windows_bootup_step_name %}

{% block windows_bootup_success_reason %}
Confirmed the presence of the expected guest agent logs in the serial console output
in GCE instance {full_resource_path}
{% endblock windows_bootup_success_reason %}

{% block windows_bootup_failure_reason %}
The expected guest agent logs are not present in the serial console output.
{% endblock windows_bootup_failure_reason %}

{% block windows_bootup_failure_remediation %}
Fix boot issues preventing a successful startup. If the Google Compute Engine (GCE) guest agents are installed, the
startup process should include the expected guest agent logs.

Resources

1. [Troubleshooting Windows instances](https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-windows)
2. [Connecting to the Windows Special Administrative Console
(SAC)](https://cloud.google.com/compute/docs/instances/connecting-to-sac)
3. [Installing the Windows GCE guest
environment](https://cloud.google.com/compute/docs/images/install-guest-environment#windows:~:text=Engine%20Shutdown%20Scripts-,Windows,-GCEGuestAgent%3A%20GCE%20Agent)
4. [Connecting to Windows instances](https://cloud.google.com/compute/docs/instances/connecting-to-windows)
5. [Connecting to Windows using SSH](https://cloud.google.com/compute/docs/connect/windows-ssh)
6. [Using PowerShell to connect to Windows
instances](https://cloud.google.com/compute/docs/instances/windows/connecting-powershell)
{% endblock windows_bootup_failure_remediation %}

{% block windows_bootup_uncertain_reason %}
Lack of serial log data prevented a thorough assessment of the VM's operational state. Result is
inconclusive
{% endblock windows_bootup_uncertain_reason %}

{% block windows_bootup_uncertain_remediation %}
Consulting the troubleshooting guide to investigate windows boot up issues: [1], [2]

Resources:

[1] <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-windows>

[2] Connecting to the Windows Special Administrative Console (SAC) to troubleshoot boot up issues: <https://cloud.google.com/compute/docs/instances/connecting-to-sac>

Google Cloud Platform Support Scope:
<https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope>
{% endblock windows_bootup_uncertain_remediation %}

{% block serial_log_start_point_step_name %}
Verify all logs available since last boot of the instance
{% endblock serial_log_start_point_step_name %}

{% block serial_log_start_point_success_reason %}
Found all logs since last boot of the VM.
{% endblock serial_log_start_point_success_reason %}

{% block serial_log_start_point_failure_reason %}
No serial console logs available since the instance's startup due to auto purge,
hence the check might not have correct results.
{% endblock serial_log_start_point_failure_reason %}

{% block serial_log_start_point_failure_remediation %}
Consider restarting the instance and then using gcpdiag to analyze the serial console logs for further insights.
{% endblock serial_log_start_point_failure_remediation %}

{% block serial_log_start_point_skipped_reason %}
There are no logs to examine.
{% endblock serial_log_start_point_skipped_reason %}

{% block time_sync_issue_step_name %}
Check for Time Sync related errors from GCE serial logs.
{% endblock time_sync_issue_step_name %}

{% block time_sync_issue_failure_reason %}
Found time sync error messages in Serial console logs. It looks like VM's time is out of syc.
{% endblock time_sync_issue_failure_reason %}

{% block time_sync_issue_failure_remediation %}
Connect to the GCE Instance and verify that the NTP server configuration adheres to Google Cloud Platform standards.
For detailed guidance, refer to the [Google Cloud NTP configuration
guide](https://cloud.google.com/compute/docs/instances/configure-ntp).
{% endblock time_sync_issue_failure_remediation %}

{% block time_sync_issue_skipped_reason %}
There are no logs to examine.
{% endblock time_sync_issue_skipped_reason %}

{% block time_sync_issue_uncertain_reason %}
No Time sync related errors in Serial console logs.
{% endblock time_sync_issue_uncertain_reason %}

{% block time_sync_issue_uncertain_remediation %}
To verify, please check your system Logs for below patterns:

```
'time may be out of sync',
'System clock is unsynchronized',
'Time drift detected',
'no servers can be used, system clock unsynchronized'
```

{% endblock time_sync_issue_uncertain_remediation %}

{% block guest_agent_step_name %}
Checking for Guest Agent startup logs
{% endblock guest_agent_step_name %}

{% block guest_agent_failure_reason %}
Could not find the Google Guest Agent startup log in Serial console logs.
{% endblock guest_agent_failure_reason %}

{% block guest_agent_failure_remediation %}
The google-guest-agent contains the guest agent and metadata script executables which
runs on the guest OS to support the Compute Engine features. These features include account
management, OS Login integration, clock skew, network interface management, and instance setup.

In case Guest Agent is not started during instance startup, users might face login issues.

The `google-guest-agent.service` service should be in running state.
If the service is disabled, enable and start the service, by running the following commands:

```
systemctl enable google-guest-agent.service
systemctl start google-guest-agent.service
```

Verify that the Linux Google Agent scripts are installed and running. If the Linux Google
Agent is not installed, re-install it.
{% endblock guest_agent_failure_remediation %}

{% block guest_agent_success_reason %}
Detected that Google Guest Agent is running within the VM
{% endblock guest_agent_success_reason %}

{% block guest_agent_skipped_reason %}
There are no logs to examine.
{% endblock guest_agent_skipped_reason %}

{% block guest_agent_uncertain_reason %}
No success or failed logs found for Google Guest Agent startup.
{% endblock guest_agent_uncertain_reason %}

{% block guest_agent_uncertain_remediation %}
Each supported operating system on Compute Engine requires specific guest environment packages.

To determine the presence of a guest environment:

- Inspect system logs emitted to the console during instance startup.
- List the installed packages while connected to the instance.

For manual validation of the Guest Agent, refer to [Installing and configuring the guest
environment](https://cloud.google.com/compute/docs/images/install-guest-environment#wgei).
{% endblock guest_agent_uncertain_remediation %}

{% block network_errors_step_name %}
Check for metadata network connectivity errors
{% endblock network_errors_step_name %}

{% block network_errors_failure_reason %}
The metadata server(169.254.169.254) is unreachable from the GCE Instance.
The instance might not have IP assigned to its primary NIC.
{% endblock network_errors_failure_reason %}

{% block network_errors_failure_remediation %}
Attempt to log in to the instance via the serial console using a password and check the status of the network stack.

If login via the serial console is unsuccessful, consider restarting the instance.

If the issue persists after a reboot, follow the [rescue VM
guide](https://cloud.google.com/compute/docs/troubleshooting/rescue-vm) for further troubleshooting.

Additionally, refer to the [troubleshooting metadata server
guide](https://cloud.google.com/compute/docs/troubleshooting/troubleshoot-metadata-server) to address potential issues
with the Compute Engine metadata server.
{% endblock network_errors_failure_remediation %}

{% block network_errors_skipped_reason %}
There are no logs to examine.
{% endblock network_errors_skipped_reason %}

{% block network_errors_uncertain_reason %}
No success or failed logs to help deduce a conlusion on certainty of Network issues on the instance.
{% endblock network_errors_uncertain_reason %}

{% block linux_fs_corruption_step_name %}
Verify any Filesystem corruption related errors in Serial console logs
{% endblock linux_fs_corruption_step_name %}

{% block linux_fs_corruption_failure_reason %}
Possible filesystem corruption detected.

The patterns used:

```
'Corruption of in-memory data detected. Shutting down filesystem',
'Corruption of in-memory data detected', 'warning: mounting fs with errors',
'Failed to mount /',
'A stop job is running for Security \.\.\..* Service ',
'I/O Error Detected. Shutting down filesystem',
'metadata I/O error in'
```

{% endblock linux_fs_corruption_failure_reason %}

{% block linux_fs_corruption_failure_remediation %}
To resolve filesystem corruption, admins can use [gce-rescue](https://github.com/GoogleCloudPlatform/gce-rescue),
available in Cloud Shell, to rescue faulty VMs. Alternatively, you can follow the
[manual method](https://cloud.google.com/compute/docs/troubleshooting/rescue-vm) to repair the filesystem.

Additional resources for reference:

- [Red Hat article on filesystem repair](https://access.redhat.com/solutions/1750923)
- [Video guide on rescuing VMs](https://www.youtube.com/watch?v=oD6IFpjEtEw)

These resources provide detailed steps for diagnosing and resolving filesystem issues.
{% endblock linux_fs_corruption_failure_remediation %}

{% block linux_fs_corruption_skipped_reason %}
There are no logs to examine.
{% endblock linux_fs_corruption_skipped_reason %}

{% block sshd_auth_failure_step_name %}
Examining SSHD authentication failures via serial logs.
{% endblock sshd_auth_failure_step_name %}

{% block sshd_auth_failure_failure_reason %}
Detected SSHD authentication issues in the GCE Instance, which is affecting SSH access.
Found the error "Authentication refused: bad ownership or modes for directory"
{% endblock sshd_auth_failure_failure_reason %}

{% block sshd_auth_failure_failure_remediation %}
To mitigate "bad ownership or modes for directory" errors:

1. Follow either of the below steps to check the permissions:
   - these steps to rescue the vm:
<https://cloud.google.com/compute/docs/troubleshooting/rescue-vm>
   - these steps login through serial console:
<https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>
2. Refer to the standard permissions required for ssh connection:
<https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh-errors#permissions>
{% endblock sshd_auth_failure_failure_remediation %}

{% block sshd_auth_failure_success_reason %}
No errors detected for sshd auth failure due to bad permissions
{% endblock sshd_auth_failure_success_reason %}

{% block sshd_auth_failure_skipped_reason %}
There are no logs to examine.
{% endblock sshd_auth_failure_skipped_reason %}

{% block sshd_auth_failure_uncertain_reason %}
No evidence of successful or failed SSHD authentication attempts is present in the serial logs.
{% endblock sshd_auth_failure_uncertain_reason %}

{% block sshd_auth_failure_uncertain_remediation %}
To check if sshd_auth_failures are happening due to wrong directory permissions:

1. Follow either of the below steps to check the permissions:
   - these steps to rescue the vm:
<https://cloud.google.com/compute/docs/troubleshooting/rescue-vm>
   - these steps login through serial console:
<https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

2. Check the system logs for following error messages:

```
'Authentication refused: bad ownership or modes for directory'
'Error updating SSH keys for user1: mkdir /home/USER/.ssh: no such file or directory'
```

{% endblock sshd_auth_failure_uncertain_remediation %}

{% block linux_fs_corruption_uncertain_reason %}
No evidence Filesystem corruption errors present in the serial logs.
{% endblock linux_fs_corruption_uncertain_reason %}

{% block linux_fs_corruption_uncertain_remediation %}

{% endblock linux_fs_corruption_uncertain_remediation %}

{% block cloud_init_startup_check_step_name %}
Checking for Cloud-init startup logs
{% endblock cloud_init_startup_check_step_name %}

{% block cloud_init_startup_check_failure_reason %}
Could not find the Cloud-init startup log in Serial console logs.
{% endblock cloud_init_startup_check_failure_reason %}

{% block cloud_init_startup_check_failure_remediation %}
Cloud-init is the standard method for initializing cloud instance when it boots up.
It is installed in official Ubuntu and Container-Optimized OS images.
Cloud-init failures can be caused by internal issues, problems with other system components, or user configuration
errors.

Check if Cloud-init process is in running state.

```
systemctl status cloud-init
```

Verify if the cloud init package is installed. Otherwise reinstall cloud-init and enable it

```
dpkg -l | grep -i cloud-init

apt-get install -f cloud-init -y
systemctl enable cloud-init
```

{% endblock cloud_init_startup_check_failure_remediation %}

{% block cloud_init_startup_check_success_reason %}
Detected that Cloud-init is running within {full_resource_path}
{% endblock cloud_init_startup_check_success_reason %}

{% block cloud_init_startup_check_skipped_reason %}
There are no logs to examine.
{% endblock cloud_init_startup_check_skipped_reason %}

{% block cloud_init_startup_check_uncertain_reason %}
No evidence of successful or failed cloud-init startup attempts in the {full_resource_path} serial logs.
{% endblock cloud_init_startup_check_uncertain_reason %}

{% block cloud_init_startup_check_uncertain_remediation %}
Check if cloud-init package is installed in VM

```
dpkg -l | grep -i cloud-init
systemctl enable cloud-init
```

{% endblock cloud_init_startup_check_uncertain_remediation %}

{% block cloud_init_step_name %}
Verify Network interface received IP through cloud-init
{% endblock cloud_init_step_name %}

{% block cloud_init_failure_reason %}
NIC did not received any IP through cloud-init
{% endblock cloud_init_failure_reason %}

{% block cloud_init_failure_remediation %}
Cloud-init is the standard method for initializing cloud instance when it boots up.
It is installed in official Ubuntu and Container-Optimized OS images.
Cloud-init failures can be caused by internal issues, problems with other system components, or user configuration
errors.

Check if Cloud-init process is in running state.

```
systemctl status cloud-init
```

Verify if the cloud init package is installed. Otherwise reinstall cloud-init and enable it

```
dpkg -l | grep -i cloud-init

apt-get install -f cloud-init -y
systemctl enable cloud-init
```

{% endblock cloud_init_failure_remediation %}

{% block cloud_init_success_reason %}
Detected that NIC has received IP through cloud-init
{% endblock cloud_init_success_reason %}

{% block cloud_init_skipped_reason %}
There are no logs to examine.
{% endblock cloud_init_skipped_reason %}

{% block cloud_init_uncertain_reason %}
Cloud-init startup logs (both success and failure) for the resource {full_resource_path}
were not found. This may indicate missing log files, incorrect logging configuration,
or an issue with the initialization process during {start_time} - {end_time}.
{% endblock cloud_init_uncertain_reason %}

{% block cloud_init_uncertain_remediation %}
The Cloud-init package may not be installed or active on the VM {full_resource_path}.
To verify, check if the package is installed by running:

```
dpkg -l | grep -i cloud-init.
```

If the package is not installed, install it using the appropriate package manager.
Additionally, ensure that the service is enabled by running:

```
systemctl enable cloud-init.
```

{% endblock cloud_init_uncertain_remediation %}
