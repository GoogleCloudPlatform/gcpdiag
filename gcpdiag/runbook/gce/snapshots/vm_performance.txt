instance_name=faulty-linux-ssh,project_id=gcpdiag-gce-vm-performance,zone=europe-west2-a

gce/vm-performance:  Google Compute Engine VM performance checks

  This runbook is designed to assist you in investigating and understanding the underlying reasons
  behind the performance issues of your Google Compute Engine VMs within Google Cloud Platform.

  Key Investigation Areas:

    - High CPU utilisation
    - CPU Over-commitment for E2 or Sole-Tenant VMs
    - High Memory utilisation
    - Disk space high utilisation
    - High Disk IOPS utilisation
    - High Disk Throughput utilisation
    - Disk Health check
    - Disk IO latency check
    - Disk Slowness check
    - Check for Live Migrations
    - Usual Error checks in Serial console logs
  
[START]: Verify GCE Instance is in a "RUNNING" state.
[AUTOMATED STEP]: Verify GCE Instance is in a "RUNNING" state.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [OK]
     [REASON]
     The GCE Instance projects/gcpdiag-gce-faultyssh-runbook/zones/europe-west2-a/instances/faulty-linux-ssh is in RUNNING state.

[AUTOMATED STEP]: Verify VM CPU utilization is within optimal levels.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     CPU utilization on this VM has surpassed recommended operational levels,
     which may affect its performance and SSH connectivity.

     [REMEDIATION]
     Excessive CPU usage can lead to performance bottlenecks. Resizing the VM to a machine type with higher CPU capabilities may resolve the issue.

     Consult the following documentation for guidance:

     - Stopping a VM: <https://cloud.google.com/compute/docs/instances/stop-start-instance>
     - Resizing a VM: <https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud>

     Additionally, analyze Compute Engine observability metrics to pinpoint high-usage processes:

     - Accessing VM observability metrics:
       <https://cloud.google.com/compute/docs/instances/observe-monitor-vms#access_vm_observability_metrics>
     - Analyzing process utilization:
       <https://cloud.google.com/compute/docs/instances/observe-monitor-vms#process_utilization>

     If SSH is unavailable, connect via the serial console to stop offending processes:
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

[AUTOMATED STEP]: Checking if CPU is overcommited

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     CPU for the VM faulty-linux-ssh is over committed beyond acceptable limits: 0 ms/s
[AUTOMATED STEP]: Verify VM memory utilization is within optimal levels.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     Memory utilization on this VM has reached levels that may compromise its VM application performance.

     [REMEDIATION]
     Elevated memory usage can result in slow, unresponsive, or terminated applications.
     Enhance the VM's memory capacity by changing to a machine type with more memory.

     Consult the following documentation for guidance:

     - Changing machine type:
       <https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud>

     Additionally, analyze Compute Engine observability metrics to pinpoint high-usage processes:
     <https://cloud.google.com/compute/docs/instances/observe-monitor-vms#memory_utilization>

     If SSH is unavailable, connect via the serial console to mitigate the issue:
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

[AUTOMATED STEP]: Verify memory related errors in VM serial logs.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [UNCERTAIN]
     [REASON]
     Unable to investigate the high memory utilization error logs, likely due to the absence of logs.
     However, this does not eliminate the possibility of high memory usage.

     Manual verification of memory utilization on the Guest OS is recommended as a potential cause.

     [REMEDIATION]

     1. Manually investigate memory usage by accessing the Guest OS:
        - Identify processes with consistently high memory consumption using `top` (press "M") or `ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -n 5`.
        - Focus on processes with recent spikes or consistently high memory usage.
        - If SSH access is unavailable, troubleshoot via the serial console:
          <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

     2. Review recent application or configuration changes:
        - Investigate if recent deployments, updates, or configuration changes correlate with increased memory usage.

     3. Resolve identified bottlenecks:
        - For applications causing excessive memory usage, optimize their configuration or update them. Explore alternatives if optimization is insufficient.
        - Evaluate scaling up resources if high memory usage results from legitimate application demands.

     4. Increase instance memory if necessary:
        - Stop the VM and change its machine type:
          <https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud>
        - Consult the machine type documentation to select an appropriate configuration:
          <https://cloud.google.com/compute/docs/machine-types>

     **Note:** Non-Google provided application-specific issues may fall outside the support scope. Collaborate with relevant application teams for further investigation. Refer to the Google Cloud Platform support policy for details, including out-of-scope items:

     - Support and maintenance policy: <https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope>
     - Out-of-scope items: <https://cloud.google.com/compute/docs/images/support-maintenance-policy#out-of-scope_for_support>

[AUTOMATED STEP]: Verify instance disks are healthy.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     You might experience slower/poor performance with your disk 'persistent-disk-0' due to an
     ongoing issue with our Compute Engine or Persistent Disk infrastructure. We're working
     to resolve this as quickly as possible.

     [REMEDIATION]
     To better understand the situation with your Compute Engine or Persistent Disks,
     could you please take a look at the Google Cloud Status page:

     <https://status.cloud.google.com>

     This page provides real-time updates on the health of Google Cloud services.

     Additionally, it may be helpful to check the Service Health dashboard in your
     Google Cloud Console for any reported incidents:

     <https://console.cloud.google.com/servicehealth/incidents>

     If you don't find any information about an ongoing issue related to your concern,
     please don't hesitate to reach out to Google Cloud Support by creating a support case.
     They'll be happy to investigate further and assist you.

[AUTOMATED STEP]: Verify VM's boot disk space utilization is within optimal levels.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     Disk utilization on this VM's boot disk is critically high,
     potentially affecting application performance.

     [REMEDIATION]
     To mitigate high disk usage, consider expanding the VM's boot disk capacity.
     This action can help avoid performance issues and ensure smoother SSH connections.

     Consult the following guide to increase disk size:
     <https://cloud.google.com/compute/docs/disks/resize-persistent-disk#increase_the_size_of_a_disk>

[AUTOMATED STEP]: Verify any slow Disk operations related errors in Serial console logs

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [UNCERTAIN]
     [REASON]
     No error messages related to disk latency were found in the serial console logs.
     This does not rule out disk performance issues.

     [REMEDIATION]
     There can be multiple reasons which can cause Slow Disk IOs:

     - CPU Starvation - Small instances (with lesser CPUs) may not have enough CPU to serve all I/Os inflight.
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#cpu_and_memory_performance>

     - Network Throttling - High sent/received network traffic can cause network throttling that impacts disk operations.
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#network_performance>

     - Insufficient Machine Resources - If your machine's IOPS and throughput limits are not enough to serve your workloads,
     this can also cause CPU or Disk IOPS/throughput Starvation.
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#storage_performance>

     - Application and GuestOS Operations - Unmanaged and untested application workloads can cause the high influx of IOs
     to the disk and cause IO operations to be queued, causing throttling at disk and CPU levels.

     To fix this issue:

     - Please optimize your application workloads.
     - If needed, please add more resources(CPU, Memory) to the VM.
     - Please optimize your Disk performance -
     <https://cloud.google.com/compute/docs/disks/optimizing-pd-performance>
     - If needed, please change your disk type to get better Disk IOPS/throughput limits -
     <https://cloud.google.com/compute/docs/disks/modify-persistent-disk#disk_type>

[AUTOMATED STEP]: Verify any Filesystem corruption related errors in Serial console logs

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     Possible filesystem corruption detected.

     The patterns used:

     ```
     'Corruption of in-memory data detected. Shutting down filesystem',
     'Corruption of in-memory data detected', 'warning: mounting fs with errors',
     'Failed to mount /',
     'A stop job is running for Security \.\.\..* Service ',
     'I/O Error Detected. Shutting down filesystem',
     'metadata I/O error in'
     ```


     [REMEDIATION]
     To resolve filesystem corruption, admins can use [gce-rescue](https://github.com/GoogleCloudPlatform/gce-rescue),
     available in Cloud Shell, to rescue faulty VMs. Alternatively, you can follow the
     [manual method](https://cloud.google.com/compute/docs/troubleshooting/rescue-vm) to repair the filesystem.

     Additional resources for reference:

     - [Red Hat article on filesystem repair](https://access.redhat.com/solutions/1750923)
     - [Video guide on rescuing VMs](https://www.youtube.com/watch?v=oD6IFpjEtEw)

     These resources provide detailed steps for diagnosing and resolving filesystem issues.

[AUTOMATED STEP]: Verify Instance's Disk Avg IO Latency is within optimal limits.

   - gcpdiag-gce-vm-performance/faulty-linux-ssh                          [FAIL]
     [REASON]
     The performance of the disk 'faulty-linux-ssh' is currently degraded due to high
     IO latency exceeding optimal thresholds. This may result in slower read/write
     speeds and overall reduced performance.

     [REMEDIATION]
     Disk I/O latency is the time it takes for a read or write operation to complete on a
     disk.
     High disk I/O latency can significantly impact the performance of your applications
     and workloads running on the instance, leading to slow response times, increased
     processing time, and overall sluggishness.

     **Potential Bottlenecks**

     - Disk Type: To optimize disk performance, ensure your disk type is appropriate
     for your workload and provides acceptable latency for your system architecture.
     Choosing the right disk type can significantly impact performance.
     <https://cloud.google.com/compute/docs/disks>

     - Workload: The nature of your workload also influences latency. Workloads with
     many small, random I/O operations will generally have higher latency than those
     with sequential I/O

     **Optimize Disk Usage**

     - Reduce I/O Operations: Optimize your applications and database queries to minimize
     the number of disk I/O operations.
     - Increase I/O Request Size: Larger I/O requests can be more efficient than many small
     ones. Consider adjusting your application or database settings to increase the I/O
     request size.
     - Caching: Implement caching mechanisms to reduce the need to access the disk for
     frequently used data.

     Choose the Right Disk Type with lesser IO Latency - <https://cloud.google.com/compute/docs/disks>

     You may also look into Optimizing persistent disk performance -
     <https://cloud.google.com/compute/docs/disks/optimizing-pd-performance>

     Please don't hesitate to reach out to Google Cloud Support if issue is not resolved.

[AUTOMATED STEP]: Verify live migrations for the instance
[INFO]: 

Live Migration Detected at 2021/11/24 16:29:21, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:34:35, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:34:37, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:34:38, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:35:08, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:35:10, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:35:22, Checking further


[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[END]: Finalize VM performance diagnostics.


instance_name=faulty-windows-ssh,project_id=gcpdiag-gce-vm-performance,zone=europe-west2-a

gce/vm-performance:  Google Compute Engine VM performance checks

  This runbook is designed to assist you in investigating and understanding the underlying reasons
  behind the performance issues of your Google Compute Engine VMs within Google Cloud Platform.

  Key Investigation Areas:

    - High CPU utilisation
    - CPU Over-commitment for E2 or Sole-Tenant VMs
    - High Memory utilisation
    - Disk space high utilisation
    - High Disk IOPS utilisation
    - High Disk Throughput utilisation
    - Disk Health check
    - Disk IO latency check
    - Disk Slowness check
    - Check for Live Migrations
    - Usual Error checks in Serial console logs
  
[START]: Verify GCE Instance is in a "RUNNING" state.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     The GCE Instance projects/gcpdiag-gce-faultyssh-runbook/zones/europe-west2-a/instances/faulty-windows-ssh is in TERMINATED state.

     [REMEDIATION]
     Restart VM projects/gcpdiag-gce-faultyssh-runbook/zones/europe-west2-a/instances/faulty-windows-ssh and ensure VM lifecycle transitions from TERMINATED to RUNNING.

     Consult the following documentation:

     - Restarting a compute instance:
       <https://cloud.google.com/compute/docs/instances/stop-start-instance#restart-vm>
     - Troubleshooting VM startup issues:
       <https://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting>

[AUTOMATED STEP]: Verify GCE Instance is in a "RUNNING" state.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     The GCE Instance projects/gcpdiag-gce-faultyssh-runbook/zones/europe-west2-a/instances/faulty-windows-ssh is in TERMINATED state.

     [REMEDIATION]
     Restart VM projects/gcpdiag-gce-faultyssh-runbook/zones/europe-west2-a/instances/faulty-windows-ssh and ensure VM lifecycle transitions from TERMINATED to RUNNING.

     Consult the following documentation:

     - Restarting a compute instance:
       <https://cloud.google.com/compute/docs/instances/stop-start-instance#restart-vm>
     - Troubleshooting VM startup issues:
       <https://cloud.google.com/compute/docs/troubleshooting/vm-startup#identify_the_reason_why_the_boot_disk_isnt_booting>

[AUTOMATED STEP]: Verify VM CPU utilization is within optimal levels.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     CPU utilization on this VM has surpassed recommended operational levels,
     which may affect its performance and SSH connectivity.

     [REMEDIATION]
     Excessive CPU usage can lead to performance bottlenecks. Resizing the VM to a machine type with higher CPU capabilities may resolve the issue.

     Consult the following documentation for guidance:

     - Stopping a VM: <https://cloud.google.com/compute/docs/instances/stop-start-instance>
     - Resizing a VM: <https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud>

     Additionally, analyze Compute Engine observability metrics to pinpoint high-usage processes:

     - Accessing VM observability metrics:
       <https://cloud.google.com/compute/docs/instances/observe-monitor-vms#access_vm_observability_metrics>
     - Analyzing process utilization:
       <https://cloud.google.com/compute/docs/instances/observe-monitor-vms#process_utilization>

     If SSH is unavailable, connect via the serial console to stop offending processes:
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

[AUTOMATED STEP]: Checking if CPU is overcommited

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     CPU for the VM faulty-windows-ssh is over committed beyond acceptable limits: 0 ms/s
[AUTOMATED STEP]: Verify VM memory utilization is within optimal levels.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     Memory utilization on this VM has reached levels that may compromise its VM application performance.

     [REMEDIATION]
     Elevated memory usage can result in slow, unresponsive, or terminated applications.
     Enhance the VM's memory capacity by changing to a machine type with more memory.

     Consult the following documentation for guidance:

     - Changing machine type:
       <https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud>

     Additionally, analyze Compute Engine observability metrics to pinpoint high-usage processes:
     <https://cloud.google.com/compute/docs/instances/observe-monitor-vms#memory_utilization>

     If SSH is unavailable, connect via the serial console to mitigate the issue:
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

[AUTOMATED STEP]: Verify memory related errors in VM serial logs.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [UNCERTAIN]
     [REASON]
     Unable to investigate the high memory utilization error logs, likely due to the absence of logs.
     However, this does not eliminate the possibility of high memory usage.

     Manual verification of memory utilization on the Guest OS is recommended as a potential cause.

     [REMEDIATION]

     1. Manually investigate memory usage by accessing the Guest OS:
        - Identify processes with consistently high memory consumption using `top` (press "M") or `ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -n 5`.
        - Focus on processes with recent spikes or consistently high memory usage.
        - If SSH access is unavailable, troubleshoot via the serial console:
          <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console>

     2. Review recent application or configuration changes:
        - Investigate if recent deployments, updates, or configuration changes correlate with increased memory usage.

     3. Resolve identified bottlenecks:
        - For applications causing excessive memory usage, optimize their configuration or update them. Explore alternatives if optimization is insufficient.
        - Evaluate scaling up resources if high memory usage results from legitimate application demands.

     4. Increase instance memory if necessary:
        - Stop the VM and change its machine type:
          <https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance#gcloud>
        - Consult the machine type documentation to select an appropriate configuration:
          <https://cloud.google.com/compute/docs/machine-types>

     **Note:** Non-Google provided application-specific issues may fall outside the support scope. Collaborate with relevant application teams for further investigation. Refer to the Google Cloud Platform support policy for details, including out-of-scope items:

     - Support and maintenance policy: <https://cloud.google.com/compute/docs/images/support-maintenance-policy#support-scope>
     - Out-of-scope items: <https://cloud.google.com/compute/docs/images/support-maintenance-policy#out-of-scope_for_support>

[AUTOMATED STEP]: Verify instance disks are healthy.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     You might experience slower/poor performance with your disk 'persistent-disk-0' due to an
     ongoing issue with our Compute Engine or Persistent Disk infrastructure. We're working
     to resolve this as quickly as possible.

     [REMEDIATION]
     To better understand the situation with your Compute Engine or Persistent Disks,
     could you please take a look at the Google Cloud Status page:

     <https://status.cloud.google.com>

     This page provides real-time updates on the health of Google Cloud services.

     Additionally, it may be helpful to check the Service Health dashboard in your
     Google Cloud Console for any reported incidents:

     <https://console.cloud.google.com/servicehealth/incidents>

     If you don't find any information about an ongoing issue related to your concern,
     please don't hesitate to reach out to Google Cloud Support by creating a support case.
     They'll be happy to investigate further and assist you.

[AUTOMATED STEP]: Verify VM's boot disk space utilization is within optimal levels.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     Disk utilization on this VM's boot disk is critically high,
     potentially affecting application performance.

     [REMEDIATION]
     To mitigate high disk usage, consider expanding the VM's boot disk capacity.
     This action can help avoid performance issues and ensure smoother SSH connections.

     Consult the following guide to increase disk size:
     <https://cloud.google.com/compute/docs/disks/resize-persistent-disk#increase_the_size_of_a_disk>

[AUTOMATED STEP]: Verify any slow Disk operations related errors in Serial console logs

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [UNCERTAIN]
     [REASON]
     No error messages related to disk latency were found in the serial console logs.
     This does not rule out disk performance issues.

     [REMEDIATION]
     There can be multiple reasons which can cause Slow Disk IOs:

     - CPU Starvation - Small instances (with lesser CPUs) may not have enough CPU to serve all I/Os inflight.
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#cpu_and_memory_performance>

     - Network Throttling - High sent/received network traffic can cause network throttling that impacts disk operations.
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#network_performance>

     - Insufficient Machine Resources - If your machine's IOPS and throughput limits are not enough to serve your workloads,
     this can also cause CPU or Disk IOPS/throughput Starvation.
     <https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-performance#storage_performance>

     - Application and GuestOS Operations - Unmanaged and untested application workloads can cause the high influx of IOs
     to the disk and cause IO operations to be queued, causing throttling at disk and CPU levels.

     To fix this issue:

     - Please optimize your application workloads.
     - If needed, please add more resources(CPU, Memory) to the VM.
     - Please optimize your Disk performance -
     <https://cloud.google.com/compute/docs/disks/optimizing-pd-performance>
     - If needed, please change your disk type to get better Disk IOPS/throughput limits -
     <https://cloud.google.com/compute/docs/disks/modify-persistent-disk#disk_type>

[AUTOMATED STEP]: Verify any Filesystem corruption related errors in Serial console logs

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [UNCERTAIN]
     [REASON]
     No evidence Filesystem corruption errors present in the serial logs.

     [REMEDIATION]
     To resolve filesystem corruption, admins can use [gce-rescue](https://github.com/GoogleCloudPlatform/gce-rescue),
     available in Cloud Shell, to rescue faulty VMs. Alternatively, you can follow the
     [manual method](https://cloud.google.com/compute/docs/troubleshooting/rescue-vm) to repair the filesystem.

     Additional resources for reference:

     - [Red Hat article on filesystem repair](https://access.redhat.com/solutions/1750923)
     - [Video guide on rescuing VMs](https://www.youtube.com/watch?v=oD6IFpjEtEw)

     These resources provide detailed steps for diagnosing and resolving filesystem issues.

[AUTOMATED STEP]: Verify Instance's Disk Avg IO Latency is within optimal limits.

   - gcpdiag-gce-vm-performance/faulty-windows-ssh                        [FAIL]
     [REASON]
     The performance of the disk 'faulty-windows-ssh' is currently degraded due to high
     IO latency exceeding optimal thresholds. This may result in slower read/write
     speeds and overall reduced performance.

     [REMEDIATION]
     Disk I/O latency is the time it takes for a read or write operation to complete on a
     disk.
     High disk I/O latency can significantly impact the performance of your applications
     and workloads running on the instance, leading to slow response times, increased
     processing time, and overall sluggishness.

     **Potential Bottlenecks**

     - Disk Type: To optimize disk performance, ensure your disk type is appropriate
     for your workload and provides acceptable latency for your system architecture.
     Choosing the right disk type can significantly impact performance.
     <https://cloud.google.com/compute/docs/disks>

     - Workload: The nature of your workload also influences latency. Workloads with
     many small, random I/O operations will generally have higher latency than those
     with sequential I/O

     **Optimize Disk Usage**

     - Reduce I/O Operations: Optimize your applications and database queries to minimize
     the number of disk I/O operations.
     - Increase I/O Request Size: Larger I/O requests can be more efficient than many small
     ones. Consider adjusting your application or database settings to increase the I/O
     request size.
     - Caching: Implement caching mechanisms to reduce the need to access the disk for
     frequently used data.

     Choose the Right Disk Type with lesser IO Latency - <https://cloud.google.com/compute/docs/disks>

     You may also look into Optimizing persistent disk performance -
     <https://cloud.google.com/compute/docs/disks/optimizing-pd-performance>

     Please don't hesitate to reach out to Google Cloud Support if issue is not resolved.

[AUTOMATED STEP]: Verify live migrations for the instance
[INFO]: 

Live Migration Detected at 2021/11/24 16:29:21, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:34:35, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:34:37, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:34:38, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:35:08, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:35:10, Checking further


[INFO]: 

Live Migration Detected at 2021/11/24 16:35:22, Checking further


[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[AUTOMATED STEP]: Verify Disk IOPS/Throughput usage is within optimal limits
[END]: Finalize VM performance diagnostics.


